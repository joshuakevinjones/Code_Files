# JKJ 2018/05/08 ~ let's load the packages all in one go like this
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "RWeka", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2")
lapply(x, require, character.only = TRUE)
# custom function to strip empty columns.  used in the sentiment analysis section below
has_data <- function(x) { sum(!is.na(x)) > 0 }
# JKJ 2018/05/08 ~ changed to let the user specify the directory
folder_to_read <- choose.dir()
# check
folder_to_read
# # JKJ 2018/05/08 ~ changed to show all arguments in case we want to use these options in the future
file_list <- list.files(
path = folder_to_read,
pattern = ".txt",
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = FALSE,
no.. = FALSE)
# check
filelist   # Returns the files in the folder
# check
file_list   # Returns the files in the folder
# make any adjustments to file_list after checking
file_list <- select(file_list, -starts_with("con"))
# JKJ 2018/05/08 ~ let's load the packages all in one go like this
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "RWeka", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
lapply(x, require, character.only = TRUE)
doParallel::registerDoParallel(cores = 4)   # loads doParallel package
# check
file_list   # Returns the files in the folder
# check
file_list   # Returns the files in the folder
# make any adjustments to file_list after checking
file_list <- select(file_list, -starts_with("con"))
str(file_list)
# lists the files to read in
datalist = lapply(file_list, function(x) readtext(x))  # datalist is a list object
# check
file_list   # Returns the files in the folder
# lists the files to read in
datalist = lapply(file_list, function(x) readtext(x))  # datalist is a list object
# lists the files to read in
datalist = lapply(file_list, function(x) readtext(x))  # datalist is a list object
# # JKJ 2018/05/08 ~ changed to show all arguments in case we want to use these options in the future
file_list <- list.files(
path = folder_to_read,
pattern = ".txt",
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = FALSE,
no.. = FALSE)
# check
file_list   # Returns the files in the folder
# make any adjustments to file_list after checking
file_list <- file_list[[-1]]
# make any adjustments to file_list after checking
file_list <- file_list[2:12]
# check
file_list   # Returns the files in the folder
# lists the files to read in
datalist = lapply(file_list, function(x) readtext(x))  # datalist is a list object
# check
datalist
str(folder_to_read)
require(readtext)
# lists the files to read in
datalist = lapply(file_list, function(x) readtext(x))  # datalist is a list object
setwd(choose.dir()) # JKJ 2018/05/08 ~ alternate way to choose the folder
# # JKJ 2018/05/08 ~ changed to show all arguments in case we want to use these options in the future
file_list <- list.files(
path = getwd(),
pattern = ".txt",
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = FALSE,
no.. = FALSE)
# check
file_list   # Returns the files in the folder
# lists the files to read in
datalist = lapply(file_list, function(x) readtext(x))  # datalist is a list object
# check
datalist
str(datalist)
glimpse(datalist)
View(datalist)
str_replace_all(datalist, "â", "")
# check
datalist
# assuming the same header/columns for all files
datafr = do.call("rbind", datalist)
# check
View(datafr)
# test for replacing garbage strings / characters
str_replace_all(datalist, "â", "â€")
# assuming the same header/columns for all files
datafr = do.call("rbind", datalist)
# check
View(datafr)
# test for replacing garbage strings / characters
str_replace_all(datalist$text, "â", "â€")
# test for replacing garbage strings / characters
str_replace_all(datalist$text, "â", "â€", "the")
install.packages("rjson")
library(rjson)
json_data <- fromJSON(paste(readLines(file.choose()), collapse=""))
View(json_data)
head(json_data)
colnames(json_data)
install.packages("jsonlite")
library(jsonlite)
json_data <- fromJSON(file.choose(), flatten = TRUE)
View(json_data)
colnames(json_data)
json_data <- fromJSON(file.choose(), flatten = TRUE)
json_data <- fromJSON(file.choose(), flatten = FALSE)
json_data <- fromJSON(paste(readLines(file.choose()), collapse=""))
json_data <- fromJSON(file=json_file)
json_data <- fromJSON(file = file.choose())
json_data <- fromJSON(file.choose())
json_data <- fromJSON(paste(readLines(json_file), collapse=""))
json_data <- fromJSON(paste(readLines(file.choose()), collapse=""))
json_data <- fromJSON(paste(readLines(file.choose()), collapse=""))
# JKJ 2018/05/08 ~ let's load the packages all in one go like this
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "RWeka", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
lapply(x, require, character.only = TRUE)
doParallel::registerDoParallel(cores = 4)   # loads doParallel package
setwd(choose.dir()) # JKJ 2018/05/08 ~ alternate way to choose the folder
# check
folder_to_read
# JKJ 2018/05/08 ~ changed to let the user specify the directory
folder_to_read <- getwd() # choose.dir()
# check
folder_to_read
# # JKJ 2018/05/08 ~ changed to show all arguments in case we want to use these options in the future
file_list <- list.files(
path = folder_to_read, # JKJ ~ need to fix so that we can use folder_to_read here
pattern = ".txt",
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# check
file_list   # Returns the files in the folder
# lists the files to read in
datalist = lapply(file_list, function(x) readtext(x))  # datalist is a list object
# check
datalist
# assuming the same header/columns for all files
datafr = do.call("rbind", datalist)
# check
View(datafr)
# test for replacing garbage strings / characters
str_replace_all(datalist$text, "â", "â€", "the")
# test for replacing garbage strings / characters
str_replace_all(datalist$text, "â", "")
# test for replacing garbage strings / characters
str_replace_all(datalist$text, "the", "")
# assuming the same header/columns for all files
datafr = do.call("rbind", datalist)
# check
View(datafr)
# creates a data corpus from the text column of the datafr data frame
datacorpus=Corpus(VectorSource(datafr$text))
# JKJ 2018/05/08 ~ added this option that reads in the files directly to a corpus
# tm pkg
datafr_corpus <- VCorpus(DirSource("texts"))
# JKJ 2018/05/08 ~ added this option that reads in the files directly to a corpus
# tm pkg
datafr_corpus <- VCorpus(file_list)
# JKJ 2018/05/08 ~ added this option that reads in the files directly to a corpus
# tm pkg
datafr_corpus <- VCorpus(datalist)
# JKJ 2018/05/08 ~ added this option that reads in the files directly to a corpus
# tm pkg
datafr_corpus <- VCorpus(datalist)
# JKJ 2018/05/08 ~ added this option that reads in the files directly to a corpus
# tm pkg
datafr_corpus <- VCorpus(DirSource(.))
# JKJ 2018/05/08 ~ added this option that reads in the files directly to a corpus
# tm pkg
datafr_corpus <- VCorpus(DirSource(~))
# JKJ 2018/05/08 ~ added this option that reads in the files directly to a corpus
# tm pkg
datafr_corpus <- VCorpus(DirSource(choose.dir()))
View(datafr_corpus)
View(datacorpus)
jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
View(jeopCorpus)
str(jeopCorpus)
jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
jeopCorpus <- tm_map(jeopCorpus, tolower)
jeopCorpus <- tm_map(jeopCorpus,removePunctuation)
#jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeMostPunctuation),preserve_intra_word_dashes = TRUE)
jeopCorpus <- tm_map(jeopCorpus, removeNumbers)
jeopCorpus <- tm_map(jeopCorpus, removeWords,stopwords('english'))
jeopCorpus <- tm_map(jeopCorpus, stripWhitespace)
jeopCorpus <- tm_map(jeopCorpus, stemDocument,language="english")
# create a copy
jeopCorpus.copy=jeopCorpus
# previous transformations turned the corpus of texts to character.  now we need to change back to a vector source
jeopCorpus.copy <- Corpus(VectorSource(jeopCorpus.copy))
# declare the doctmtrx object
doctmtrx=NULL
# make the document term matrix using this object
doctmtrx=DocumentTermMatrix(jeopCorpus.copy)
# TO DO ... document what this does
alarm()
# check
class(doctmtrx)
# TO DO ... document what this does
dtm <- as.matrix(doctmtrx)
# JKJ 2018/0508 ~ added a secondary version of the term summary
# use tidytext pkg to convert tm pkg DocumentTermMatrix into tidy format
dtm_df <- tidy(datafr_corpus)
dtm_df %>%
unnest_tokens(word, text) %>% # unnest tokens automatically converts to lowercase
select_if(has_data) %>% # strip empty columns
count(word, sort = TRUE) %>%
top_n(10)
# custom function to strip empty columns.  used in the sentiment analysis section below
has_data <- function(x) { sum(!is.na(x)) > 0 }
dtm_df %>%
unnest_tokens(word, text) %>% # unnest tokens automatically converts to lowercase
select_if(has_data) %>% # strip empty columns
count(word, sort = TRUE) %>%
top_n(10)
# JKJ 2018/05/08 ~ added the code below to show my version for a wordcloud
word_data_1=as.data.frame(cbind.data.frame(Words = colnames(dtm), freq=colSums(dtm)))
word_data_1 %>%
count(Words, sort = TRUE) %>%
with(wordcloud(words=colnames(dtm),freq=colSums(dtm), max.words = 35, min.freq=10, scale=c(3, 0.1), colors=brewer.pal(n=10, "Paired"), random.order=TRUE))
str(dtm_df)
# To find the top N words - Here N is the threshold freq
findMostFreqTerms(doctmtrx) #Gives most frequent occuring words in each document
findFreqTerms(doctmtrx, 100) #atleast frequency of 100
### Tarun bigrams
bigrams = textcnt(datafr$text, n = 2, method = "string")
bigrams = bigrams[order(bigrams, decreasing = TRUE)]
View(bigrams)
# use the VCorpus object loaded via tm package
docs <- VCorpus(DirSource("texts"))
dtm_df %>%
unnest_tokens(word, text) %>% # unnest tokens automatically converts to lowercase
select_if(has_data) %>% # strip empty columns
count(word, sort = TRUE) %>%
top_n(10)
dtm_df %>%
unnest_tokens(word, text) %>% # unnest tokens automatically converts to lowercase
select_if(has_data) %>% # strip empty columns
count(word, sort = TRUE) %>%
bottom_n(10)
dtm_df %>%
unnest_tokens(word, text) %>% # unnest tokens automatically converts to lowercase
select_if(has_data) %>% # strip empty columns
count(word, sort = TRUE) %>%
btm_n(10)
dtm_df %>%
unnest_tokens(word, text) %>% # unnest tokens automatically converts to lowercase
select_if(has_data) %>% # strip empty columns
count(word, sort = TRUE) %>%
top_n(10, -n)
dtm_df %>%
unnest_tokens(word, text) %>% # unnest tokens automatically converts to lowercase
select_if(has_data) %>% # strip empty columns
count(word, sort = TRUE) %>%
top_n(10)
# JKJ 2018/0508 ~ added a secondary version of the term summary
# use tidytext pkg to convert tm pkg DocumentTermMatrix into tidy format
#dtm_df <- tidy(datafr_corpus)
dtm_df <- tidy(jeopCorpus.copy)
word_data_1 <- word_data_1[order(-word_data_1$freq),]
word_data_1
# use the VCorpus object loaded via tm package
docs <- VCorpus(DirSource("texts"))
# use the VCorpus object loaded via tm package
docs <- VCorpus(DirSource(dir.choose()))
# use the VCorpus object loaded via tm package
docs <- VCorpus(DirSource(choose.dir()))
quantedaCorpus <- corpus(docs)
rm(docs) # cleanup
# check structure
summary(quantedaCorpus, showmeta = TRUE)
# note: it is necessary to manually specify the word.  See "America" below.
kwic(quantedaCorpus, "America")
docDfm <- dfm(quantedaCorpus, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
docSim <- textstat_simil(docDfm, "Trump CIA Speech.txt", margin = "documents", method = "cosine")
docSim # need a way to sort this descending !!!
# note: it is necessary to manually specify the word.  See "America" below.
kwic(quantedaCorpus, "China")
