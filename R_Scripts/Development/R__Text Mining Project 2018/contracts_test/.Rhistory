)
# creates a data corpus from the text column of the datafr data frame
#datacorpus=Corpus(VectorSource(datafr$text))
# old way
datacorpus=Corpus(VectorSource(datalist$text))
## start of transformations (all done on datacorpus object)
jeopCorpus=NULL
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
jeopCorpus <- tm_map(jeopCorpus, tolower)
jeopCorpus <- tm_map(jeopCorpus,removePunctuation)
#jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeMostPunctuation),preserve_intra_word_dashes = TRUE)
jeopCorpus <- tm_map(jeopCorpus, removeNumbers)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeURL))
jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords('english'))
jeopCorpus <- tm_map(jeopCorpus, removeWords, c('said', 'asked'))
jeopCorpus <- tm_map(jeopCorpus, removeWords, c("said", "asked"))
jeopCorpus <- tm_map(jeopCorpus, removeWords, "said")
wordlist <- c("said", "shall")
jeopCorpus <- tm_map(jeopCorpus, removeWords, wordlist)
# make the document term matrix using this object
doctmtrx=DocumentTermMatrix(jeopCorpus)  # JKJ 2018/05/17 ~ changed from jeopCorpus.copy
# TO DO ... document what this does
dtm <- as.matrix(doctmtrx)
## PREP
# Term summary (done on dtm object)
word_data_1=as.data.frame(cbind.data.frame(Words = colnames(dtm), freq=colSums(dtm)))
#names(word_data_1)
# put in descending order
word_data_1 <- word_data_1[order(-word_data_1$freq),]
# check
word_data_1
# Optional: write to .csv
#write.csv(word_data_1,"Summary.csv",row.names=F)
# Wordcloud
library(wordcloud)
wordcloud(
words=colnames(dtm),
freq=colSums(dtm),
max.words = 50,
min.freq = 5,
scale=c(5, .3),
colors=brewer.pal(n=8, "Paired"),
random.order=TRUE,  # spreads out the words in the plot
rot.per = .35  # determines the fraction of words plotted vertically
)
#install.packages("wordcloud2")
library(wordcloud2)
wordcloud2(
data = word_data_1,
size = .8,
shape = "oval",
fontFamily = "CMU Sans Serif",
rotateRatio = .5,
ellipticity = .9,
color = brewer.pal(n=12, "Paired"),
)
# creates a data corpus from the text column of the datafr data frame
#datacorpus=Corpus(VectorSource(datafr$text))
# old way
datacorpus=Corpus(VectorSource(datalist$text))
## start of transformations (all done on datacorpus object)
jeopCorpus=NULL
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
jeopCorpus <- tm_map(jeopCorpus, tolower)
jeopCorpus <- tm_map(jeopCorpus,removePunctuation)
#jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeMostPunctuation),preserve_intra_word_dashes = TRUE)
jeopCorpus <- tm_map(jeopCorpus, removeNumbers)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeURL))
jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords('english'))
# creates a data corpus from the text column of the datafr data frame
#datacorpus=Corpus(VectorSource(datafr$text))
# old way
datacorpus=Corpus(VectorSource(datalist$text))
## start of transformations (all done on datacorpus object)
jeopCorpus=NULL
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
jeopCorpus <- tm_map(jeopCorpus, tolower)
jeopCorpus <- tm_map(jeopCorpus,removePunctuation)
#jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeMostPunctuation),preserve_intra_word_dashes = TRUE)
jeopCorpus <- tm_map(jeopCorpus, removeNumbers)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeURL))
jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords('english'))
jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords('english'))
jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords("english"))
require("knitr")
opts_knit$set(root.dir = "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/")
# load the packages all in one go
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
lapply(x, require, character.only = TRUE)
doParallel::registerDoParallel(cores = 4)   # loads doParallel package
## Setup for special functions used in the analysis
# custom function to strip empty columns.  used in the sentiment analysis section below
has_data <- function(x) { sum(!is.na(x)) > 0 }
# set folder to read
# <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts"
#folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/texts"
folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/contracts_test"
# check
folder_to_read
#
file_list <- list.files(
#path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = "complex.pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# reads in the files using the readtext package
#datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object
# this was the old way, now deprecated
#datalist <- readtext(paste0("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\texts\\*.txt"))
# read pdfs
datalist <- readtext(paste0("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\contracts_test\\*Complex.pdf"))
# check
#str(datalist)  # datalist is a list of data frames
#head(datalist)  # shows the documents as 1x2 data frames
# combine all the datalist items into a single object
#datafr = do.call("rbind", datalist)
# JKJ 2018/05/30 no longer need
# creates a data corpus from the text column of the datafr data frame
#datacorpus=Corpus(VectorSource(datafr$text))
# old way
datacorpus=Corpus(VectorSource(datalist$text))
## start of transformations (all done on datacorpus object)
jeopCorpus=NULL
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
jeopCorpus <- tm_map(jeopCorpus, tolower)
jeopCorpus <- tm_map(jeopCorpus,removePunctuation)
#jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeMostPunctuation),preserve_intra_word_dashes = TRUE)
jeopCorpus <- tm_map(jeopCorpus, removeNumbers)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeURL))
jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords("english"))
jeopCorpus <- tm_map(jeopCorpus, stripWhitespace)
#jeopCorpus <- tm_map(jeopCorpus, stemDocument,language="english")
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)
jeopCorpus <- tm_map(jeopCorpus, removeSpecialChars)
# make the document term matrix using this object
doctmtrx=DocumentTermMatrix(jeopCorpus)  # JKJ 2018/05/17 ~ changed from jeopCorpus.copy
# TO DO ... document what this does
dtm <- as.matrix(doctmtrx)
## PREP
# Term summary (done on dtm object)
word_data_1=as.data.frame(cbind.data.frame(Words = colnames(dtm), freq=colSums(dtm)))
#names(word_data_1)
# put in descending order
word_data_1 <- word_data_1[order(-word_data_1$freq),]
# check
word_data_1
# Optional: write to .csv
#write.csv(word_data_1,"Summary.csv",row.names=F)
## PREP
# Term summary (done on dtm object)
word_data_1=as.data.frame(cbind.data.frame(Words = colnames(dtm), freq=colSums(dtm)))
#names(word_data_1)
# put in descending order
word_data_1 <- word_data_1[order(-word_data_1$freq),]
# check
word_data_1
# Optional: write to .csv
#write.csv(word_data_1,"Summary.csv",row.names=F)
# Wordcloud
library(wordcloud)
wordcloud(
words=colnames(dtm),
freq=colSums(dtm),
max.words = 50,
min.freq = 5,
scale=c(5, .3),
colors=brewer.pal(n=8, "Paired"),
random.order=TRUE,  # spreads out the words in the plot
rot.per = .35  # determines the fraction of words plotted vertically
)
#install.packages("wordcloud2")
library(wordcloud2)
wordcloud2(
data = word_data_1,
size = .8,
shape = "oval",
fontFamily = "CMU Sans Serif",
rotateRatio = .5,
ellipticity = .9,
color = brewer.pal(n=12, "Paired"),
)
jeopCorpus <- tm_map(jeopCorpus, removeWords, "said")
jeopCorpus <- tm_map(jeopCorpus, removeWords, "said")
# make the document term matrix using this object
doctmtrx=DocumentTermMatrix(jeopCorpus)  # JKJ 2018/05/17 ~ changed from jeopCorpus.copy
# TO DO ... document what this does
dtm <- as.matrix(doctmtrx)
## PREP
# Term summary (done on dtm object)
word_data_1=as.data.frame(cbind.data.frame(Words = colnames(dtm), freq=colSums(dtm)))
#names(word_data_1)
# put in descending order
word_data_1 <- word_data_1[order(-word_data_1$freq),]
# check
word_data_1
# Optional: write to .csv
#write.csv(word_data_1,"Summary.csv",row.names=F)
# Wordcloud
library(wordcloud)
wordcloud(
words=colnames(dtm),
freq=colSums(dtm),
max.words = 50,
min.freq = 5,
scale=c(5, .3),
colors=brewer.pal(n=8, "Paired"),
random.order=TRUE,  # spreads out the words in the plot
rot.per = .35  # determines the fraction of words plotted vertically
)
#install.packages("wordcloud2")
library(wordcloud2)
wordcloud2(
data = word_data_1,
size = .8,
shape = "oval",
fontFamily = "CMU Sans Serif",
rotateRatio = .5,
ellipticity = .9,
color = brewer.pal(n=12, "Paired"),
)
jeopCorpus <- tm_map(jeopCorpus, removeWords, c("said", "asked"))
# make the document term matrix using this object
doctmtrx=DocumentTermMatrix(jeopCorpus)  # JKJ 2018/05/17 ~ changed from jeopCorpus.copy
# TO DO ... document what this does
dtm <- as.matrix(doctmtrx)
## PREP
# Term summary (done on dtm object)
word_data_1=as.data.frame(cbind.data.frame(Words = colnames(dtm), freq=colSums(dtm)))
#names(word_data_1)
# put in descending order
word_data_1 <- word_data_1[order(-word_data_1$freq),]
# check
word_data_1
# Optional: write to .csv
#write.csv(word_data_1,"Summary.csv",row.names=F)
# Wordcloud
library(wordcloud)
wordcloud(
words=colnames(dtm),
freq=colSums(dtm),
max.words = 50,
min.freq = 5,
scale=c(5, .3),
colors=brewer.pal(n=8, "Paired"),
random.order=TRUE,  # spreads out the words in the plot
rot.per = .35  # determines the fraction of words plotted vertically
)
#install.packages("wordcloud2")
library(wordcloud2)
wordcloud2(
data = word_data_1,
size = .8,
shape = "oval",
fontFamily = "CMU Sans Serif",
rotateRatio = .5,
ellipticity = .9,
color = brewer.pal(n=12, "Paired"),
)
install.packages("knitr")
require("knitr")
opts_knit$set(root.dir = "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/")
# load the packages all in one go
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
lapply(x, require, character.only = TRUE)
doParallel::registerDoParallel(cores = 4)   # loads doParallel package
## Setup for special functions used in the analysis
# custom function to strip empty columns.  used in the sentiment analysis section below
has_data <- function(x) { sum(!is.na(x)) > 0 }
# set folder to read
# <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts"
#folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/texts"
folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/contracts_test"
# check
folder_to_read
# check
folder_to_read
#
file_list <- list.files(
#path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
require("knitr")
opts_knit$set(root.dir = "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/")
# load the packages all in one go
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
lapply(x, require, character.only = TRUE)
doParallel::registerDoParallel(cores = 4)   # loads doParallel package
## Setup for special functions used in the analysis
# custom function to strip empty columns.  used in the sentiment analysis section below
has_data <- function(x) { sum(!is.na(x)) > 0 }
# set folder to read
# <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts"
#folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/texts"
folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/contracts_test"
# check
folder_to_read
#
file_list <- list.files(
#path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
# set folder to read
# <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts"
#folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/texts"
folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/contracts_test/"
# check
folder_to_read
#
file_list <- list.files(
#path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
#
file_list <- list.files(
#path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
# set folder to read
# <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts"
folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/texts"
#
file_list <- list.files(
#path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
#
file_list <- list.files(
#path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".txt", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
#
file_list <- list.files(
#path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".*.txt", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
require("knitr")
opts_knit$set(root.dir = "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/")
# load the packages all in one go
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
lapply(x, require, character.only = TRUE)
doParallel::registerDoParallel(cores = 4)   # loads doParallel package
```{r include=FALSE}
## Setup for special functions used in the analysis
# custom function to strip empty columns.  used in the sentiment analysis section below
has_data <- function(x) { sum(!is.na(x)) > 0 }
# set folder to read
# <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts"
#folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/texts"
folder_to_read <- "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/contracts_test"
# check
folder_to_read
#
file_list <- list.files(
#path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
#
file_list <- list.files(
#path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf")
# List of files that were read in
file_list
#
file_list <- list.files(
path = "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/contracts_test",
#folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
setwd("S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/contracts_test")
library(RODBC)
library(data.table)
library(readtext)
library(tm)
library(wordcloud)
library(RWeka)
setwd("S:/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/contracts_test")   # Enter the location of the folder.
filelist = list.files(pattern = ".*.pdf")
filelist   # Returns the files in the folder
folder_to_read
filelist = list.files(path=folder_to_read,pattern = ".*.pdf")
filelist   # Returns the files in the folder
folder_to_read
filelist = list.files(path=as.character(folder_to_read),pattern = ".*.pdf")
filelist   # Returns the files in the folder
filelist = list.files(pattern = ".*.pdf")
filelist   # Returns the files in the folder
?list.files
filelist = list.files(path = folder_to_read,pattern = ".*.pdf")
filelist   # Returns the files in the folder
filelist = list.files(path = ".",pattern = ".*.pdf")
filelist   # Returns the files in the folder
#
file_list <- list.files(
path = "./contracts_test",
#folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
getwd()
opts_knit$set(root.dir = "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining/")
#
file_list <- list.files(
path = "./contracts_test",
#folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
opts_knit$set(root.dir = "S:/ACE/Projects/ACE - Ongoing and Standard Analytics/R__Text Mining")
#
file_list <- list.files(
path = "./contracts_test",
#folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
