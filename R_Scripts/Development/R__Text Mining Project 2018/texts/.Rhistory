library(rattle)
library(rattle)
rattle()
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
lapply(x, require, character.only = TRUE)
# set folder to read
folder_to_read <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\Audit Clauses"
# check
folder_to_read
#
file_list <- list.files(
path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\Audit Clauses",
#folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".txt", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
file_list
# reads in the files using the readtext package
datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object
# combine all the datalist items into a single object
datafr = do.call("rbind", datalist)
# reads in the files using the readtext package
datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object
# List of files that were read in
file_list   # Returns the files in the folder
# reads in the files using the readtext package
datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object
# reads in the files using the readtext package
datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object
# reads in the files using the readtext package
datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object
setwd("C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\Audit Clauses")
# set folder to read
folder_to_read <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\Audit Clauses"
# check
folder_to_read
#
file_list <- list.files(
path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\Audit Clauses",
#folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".txt", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
file_list
# reads in the files using the readtext package
datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object
# combine all the datalist items into a single object
datafr = do.call("rbind", datalist)
## start of transformations (all done on datacorpus object)
jeopCorpus=NULL
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
jeopCorpus <- tm_map(jeopCorpus, tolower)
jeopCorpus <- tm_map(jeopCorpus,removePunctuation)
#jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeMostPunctuation),preserve_intra_word_dashes = TRUE)
jeopCorpus <- tm_map(jeopCorpus, removeNumbers)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeURL))
jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords('english'))
jeopCorpus <- tm_map(jeopCorpus, stripWhitespace)
## TO DO ... need to remove user-specified words / phrases ...
removeGarbage <- function(x) gsub("â", "", x)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeGarbage))
removeGarbage <- function(x) gsub("???T", "", x)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeGarbage))
```
## start of transformations (all done on datacorpus object)
jeopCorpus=NULL
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
# creates a data corpus from the text column of the datafr data frame
datacorpus=Corpus(VectorSource(datafr$text))
jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
jeopCorpus <- tm_map(jeopCorpus, tolower)
jeopCorpus <- tm_map(jeopCorpus,removePunctuation)
#jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeMostPunctuation),preserve_intra_word_dashes = TRUE)
jeopCorpus <- tm_map(jeopCorpus, removeNumbers)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeURL))
jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords('english'))
jeopCorpus <- tm_map(jeopCorpus, stripWhitespace)
## TO DO ... need to remove user-specified words / phrases ...
removeGarbage <- function(x) gsub("â", "", x)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeGarbage))
removeGarbage <- function(x) gsub("???T", "", x)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeGarbage))
```
# make the document term matrix using this object
doctmtrx=DocumentTermMatrix(jeopCorpus)  # JKJ 2018/05/17 ~ changed from jeopCorpus.copy
# TO DO ... document what this does
dtm <- as.matrix(doctmtrx)
# Term summary (done on dtm object)
word_data_1=as.data.frame(cbind.data.frame(Words = colnames(dtm), freq=colSums(dtm)))
# put in descending order
word_data_1 <- word_data_1[order(-word_data_1$freq),]
# list most frequent terms (all documents)
top_n_terms <- word_data_1 %>%
select_if(has_data) %>% # strip empty columns
top_n(10)
has_data <- function(x) { sum(!is.na(x)) > 0 }
# list most frequent terms (all documents)
top_n_terms <- word_data_1 %>%
select_if(has_data) %>% # strip empty columns
top_n(10)
top_n_terms
# new way (descending order)
ggplot(top_n_terms, aes(x=reorder(Words,freq), y=freq)) + geom_bar(stat = "identity", fill="red") + xlab("Words") + ylab("Count") + ggtitle("Top 10 Terms") + coord_flip()
### BIGRAMS
bigrams = textcnt(datafr$text, n = 2, method = "string")
### BIGRAMS
bigrams = textcnt(datafr$text, n = 2, method = "string")
bigrams = bigrams[order(bigrams, decreasing = TRUE)]
# plot bar chart of bigrams
library(data.table)
d1=setDT(as.data.frame(bigrams), keep.rownames = TRUE)[]
top_n_bigrams <- d1 %>%
select_if(has_data) %>% # strip empty columns
top_n(10)
top_n_bigrams
### TRIGRAMS
trigrams = textcnt(datafr$text, n = 3, method = "string")
trigrams = trigrams[order(trigrams, decreasing = TRUE)]
# plot bar chart of trigrams
d2=setDT(as.data.frame(trigrams), keep.rownames = TRUE)[]
ggplot(d2[1:10], aes(x=reorder(rn,trigrams), y=trigrams)) + geom_bar(stat = "identity", fill="Purple") + xlab("Words") + ylab("Count") + ggtitle("Top Trigrams") + coord_flip()
## quanteda package
quantedaCorpus <- corpus(jeopCorpus)
ggplot(d1[1:10], aes(x=reorder(rn,bigrams), y=bigrams)) + geom_bar(stat = "identity", fill="Blue") + xlab("Words") + ylab("Count") + ggtitle("Top Bigrams") + coord_flip()
# new way (descending order)
ggplot(top_n_terms, aes(x=reorder(Words,freq), y=freq)) + geom_bar(stat = "identity", fill="red") + xlab("Words") + ylab("Count") + ggtitle("Top 10 Terms") + coord_flip()
# specify first keyword
words <- c("records", "such records", "a period", "audit", "right")
# show the words in context
kwic(quantedaCorpus, words)
# load the packages all in one go
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
lapply(x, require, character.only = TRUE)
doParallel::registerDoParallel(cores = 4)   # loads doParallel package
# setup for working directory
#setwd(choose.dir())  # alternate way to choose the folder
setwd("C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts")
getwd()
## Setup for special functions used in the analysis
# custom function to strip empty columns.  used in the sentiment analysis section below
has_data <- function(x) { sum(!is.na(x)) > 0 }
# set folder to read
folder_to_read <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts"
# check
folder_to_read
#
file_list <- list.files(
path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
#folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".txt", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
# reads in the files using the readtext package
datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object
# check
#str(datalist)  # datalist is a list of data frames
#head(datalist)  # shows the documents as 1x2 data frames
# combine all the datalist items into a single object
datafr = do.call("rbind", datalist)
# creates a data corpus from the text column of the datafr data frame
datacorpus=Corpus(VectorSource(datafr$text))
## start of transformations (all done on datacorpus object)
jeopCorpus=NULL
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
jeopCorpus <- tm_map(jeopCorpus, tolower)
jeopCorpus <- tm_map(jeopCorpus,removePunctuation)
#jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeMostPunctuation),preserve_intra_word_dashes = TRUE)
jeopCorpus <- tm_map(jeopCorpus, removeNumbers)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeURL))
jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords('english'))
jeopCorpus <- tm_map(jeopCorpus, stripWhitespace)
#jeopCorpus <- tm_map(jeopCorpus, stemDocument,language="english")
# make the document term matrix using this object
doctmtrx=DocumentTermMatrix(jeopCorpus)  # JKJ 2018/05/17 ~ changed from jeopCorpus.copy
# TO DO ... document what this does
dtm <- as.matrix(doctmtrx)
## PREP
# Term summary (done on dtm object)
word_data_1=as.data.frame(cbind.data.frame(Words = colnames(dtm), freq=colSums(dtm)))
#names(word_data_1)
# put in descending order
word_data_1 <- word_data_1[order(-word_data_1$freq),]
# check
word_data_1
# Optional: write to .csv
#write.csv(word_data_1,"Summary.csv",row.names=F)
# Wordcloud
wordcloud(words=colnames(dtm),freq=colSums(dtm), max.words = 30, min.freq=5, scale=c(4, .1), colors=brewer.pal(n=10, "Paired"), random.order=TRUE)
# list most frequent terms (all documents)
(top_n_terms <- word_data_1 %>%
select_if(has_data) %>% # strip empty columns
top_n(10));
# list most frequent terms (all documents)
(top_n_terms <- word_data_1 %>%
select_if(has_data) %>% # strip empty columns
top_n(10));
# new way (descending order)
ggplot(top_n_terms, aes(x=reorder(Words,freq), y=freq)) + geom_bar(stat = "identity", fill="red") + xlab("Words") + ylab("Count") + ggtitle("Top 10 Terms") + coord_flip()
bigrams = textcnt(datafr$text, n = 2, method = "string")
bigrams = bigrams[order(bigrams, decreasing = TRUE)]
# Optional: Write list of bigrams to a file
#fwrite(as.data.frame(bigrams),row.names = T,file = "Bigrams.csv") #This is to write the Bigram Summmary into an excel file.
# plot bar chart of bigrams
library(data.table)
d1=setDT(as.data.frame(bigrams), keep.rownames = TRUE)[]
(top_n_bigrams <- d1 %>%
select_if(has_data) %>% # strip empty columns
top_n(10));
(top_n_trigrams <- d2 %>%
select_if(has_data) %>% # strip empty columns
top_n(10));
# plot bar chart of trigrams
d2=setDT(as.data.frame(trigrams), keep.rownames = TRUE)
trigrams = textcnt(datafr$text, n = 3, method = "string")
trigrams = textcnt(datafr$text, n = 3, method = "string")
trigrams = trigrams[order(trigrams, decreasing = TRUE)]
# plot bar chart of trigrams
d2=setDT(as.data.frame(trigrams), keep.rownames = TRUE)
(top_n_trigrams <- d2 %>%
select_if(has_data) %>% # strip empty columns
top_n(10));
ggplot(d2[1:10], aes(x=reorder(rn,trigrams), y=trigrams)) + geom_bar(stat = "identity", fill="Purple") + xlab("Words") + ylab("Count") + ggtitle("Top Trigrams") + coord_flip()
# use the VCorpus object loaded via tm package
quantedaCorpus <- corpus(jeopCorpus)
# specify first keyword
words <- c("China")
# show the words in context
kwic(quantedaCorpus, words)
# specify second keyword
words <- c("People")
kwic(quantedaCorpus, words)
# specify first keyword
words <- c("China")
# show the words in context
kwic(quantedaCorpus, words)
# specify second keyword
words <- c("Guns")
kwic(quantedaCorpus, words)
# specify first keyword
words <- c("China")
# show the words in context
kwic(quantedaCorpus, words)
# specify second keyword
words <- c("Europe")
kwic(quantedaCorpus, words)
# specify first keyword
audit_clause <- c("records", "such records", "a period", "audit", "right")
kwic(quantedaCorpus, audit_clause)
a <- kwic(quantedaCorpus, audit_clause)
head(a)
a[4]
a[,4]
pos <- read.table("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\Positive.txt")
neg <- read.table("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\Negative.txt")
pos.matches <- match(colnames(dtm),as.character(pos$V1))
neg.matches <- match(colnames(dtm),as.character(neg$V1))
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
pos.matrix <- doctmtrx[,pos.matches]
sentiment.score <- rowSums(as.matrix(pos.matrix)) - rowSums(as.matrix(neg.matrix))
New_data=NULL
neg.matrix <- doctmtrx[,neg.matches]
sentiment.score <- rowSums(as.matrix(pos.matrix)) - rowSums(as.matrix(neg.matrix))
sentiment.score
New_data=NULL
New_data=cbind.data.frame(Doc_name=datafr[,c(1)],Sentiment_BOW=sentiment.score)
# check
head(New_data)
# legend
tbl_df(as.data.frame(New_data[1]))
# Sentiment plot by document
plot(x=row.names(New_data),y=New_data$Sentiment_BOW, type="b",xlab = "Document ID", ylab = "Sentiment")
# Alternate sentiment plot by document (superceded by next plot)
#ggplot(New_data, aes(x=row.names(New_data), y=Sentiment_BOW)) + geom_bar(stat="Identity", fill="Maroon")
# Latest sentiment plot by document
ggplot(New_data, aes(x=reorder(Doc_name,Sentiment_BOW), y=Sentiment_BOW)) + geom_bar(stat = "identity", fill="orange") + xlab("File name") + ylab("Sentiment Score") +
ggtitle("BOW Sentiment Score") + coord_flip()
## Using Syuzhet methodology ##
#install.packages('syuzhet')
library(syuzhet)
#install.packages('RNewsflow')
#install.packages('reshape2')
#install.packages('corrgram')
library(RNewsflow)
library(reshape2)
library(corrgram)
syuzhet_vector=get_sentiment(datafr$text,method="syuzhet")
New_data=cbind(New_data, Sentiment_syuzhnet=syuzhet_vector)
# check
#View(New_data)
s_v <- get_sentences(datafr$text)
s_v_sentiment <- get_sentiment(s_v)
nrc_data <- get_nrc_sentiment(datafr$text)
#View(nrc_data)
New_data=cbind.data.frame(New_data,nrc_data)
#glimpse(New_data)
#write.csv(New_data,file = "Sentiment_Analysis_Combined.csv")
# ADVANCED REQUIREMENT 3: Similarity among Documents
# we will visualize with a heatmap
# View Sentiment results table
as.data.frame(New_data)
## Using Syuzhet methodology ##
#install.packages('syuzhet')
library(syuzhet)
#install.packages('RNewsflow')
#install.packages('reshape2')
#install.packages('corrgram')
library(RNewsflow)
library(reshape2)
library(corrgram)
syuzhet_vector=get_sentiment(datafr$text,method="syuzhet")
New_data=cbind(New_data, Sentiment_syuzhnet=syuzhet_vector)
# check
#View(New_data)
s_v <- get_sentences(datafr$text)
s_v_sentiment <- get_sentiment(s_v)
nrc_data <- get_nrc_sentiment(datafr$text)
#View(nrc_data)
New_data=cbind.data.frame(New_data,nrc_data)
#glimpse(New_data)
#write.csv(New_data,file = "Sentiment_Analysis_Combined.csv")
# ADVANCED REQUIREMENT 3: Similarity among Documents
# we will visualize with a heatmap
as.data.frame(New_data)
doc_comp <- as.data.frame(documents.compare(doctmtrx ))
# legend
tbl_df(as.data.frame(New_data[1]))
doc_comp <- doc_comp %>%
arrange(desc(similarity))
doc_comp
new_doc_comp <-  reshape2::acast(doc_comp, x  ~ y, value.var = "similarity")
new_doc_comp.sorted = new_doc_comp[as.character(order(rownames(new_doc_comp))),as.character(order(rownames(new_doc_comp)))]
rm(new_doc_comp)
new_doc_comp.sorted <- as.data.frame(new_doc_comp.sorted)
# corrgram
corrgram(new_doc_comp.sorted,
#order=TRUE,
main="Document Similarity Matrix",
lower.panel=panel.shade,
upper.panel=panel.pie,
diag.panel = panel.pts,
text.panel=panel.txt)
corrgram(new_doc_comp.sorted,
lower.panel=panel.pts, upper.panel=panel.conf,
diag.panel=panel.density)
# Wordcloud
wordcloud(words=colnames(dtm),freq=colSums(dtm), max.words = 30, min.freq=5, scale=c(4, .1), colors=brewer.pal(n=10, "Paired"), random.order=TRUE)
# list most frequent terms (all documents)
(top_n_terms <- word_data_1 %>%
select_if(has_data) %>% # strip empty columns
top_n(10));
# new way (descending order)
ggplot(top_n_terms, aes(x=reorder(Words,freq), y=freq)) + geom_bar(stat = "identity", fill="red") + xlab("Words") + ylab("Count") + ggtitle("Top 10 Terms") + coord_flip()
(top_n_bigrams <- d1 %>%
select_if(has_data) %>% # strip empty columns
top_n(10));
ggplot(d1[1:10], aes(x=reorder(rn,bigrams), y=bigrams)) + geom_bar(stat = "identity", fill="Blue") + xlab("Words") + ylab("Count") + ggtitle("Top Bigrams") + coord_flip()
trigrams = textcnt(datafr$text, n = 3, method = "string")
trigrams = trigrams[order(trigrams, decreasing = TRUE)]
# View trigrams
#head(as.data.frame(trigrams), 20)
# Optional: Write list of trigrams to a file
#fwrite(as.data.frame(trigrams),row.names = T,file = "Trigrams.csv") #This is to write the Trigram Summmary into an excel file.
# plot bar chart of trigrams
d2=setDT(as.data.frame(trigrams), keep.rownames = TRUE)
(top_n_trigrams <- d2 %>%
select_if(has_data) %>% # strip empty columns
top_n(10));
ggplot(d2[1:10], aes(x=reorder(rn,trigrams), y=trigrams)) + geom_bar(stat = "identity", fill="Purple") + xlab("Words") + ylab("Count") + ggtitle("Top Trigrams") + coord_flip()
# use the VCorpus object loaded via tm package
quantedaCorpus <- corpus(jeopCorpus)
# specify first keyword
words <- c("China")
# show the words in context
kwic(quantedaCorpus, words)
# specify second keyword
words <- c("Europe")
kwic(quantedaCorpus, words)
# specify first keyword
audit_clause <- c("records", "such records", "a period", "audit", "right")
kwic(quantedaCorpus, audit_clause)
# specify first keyword
audit_clause <- c("records", "such records", "a period", "audit", "right")
kwic(quantedaCorpus, audit_clause)
####### Using Bag of Words and calculating the overall Sentiment of all the document ####################
#pos = read.table("S:/Projects/2018 R Text Mining/Positive.txt", sep="\n")
pos <- read.table("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\Positive.txt")
#View(pos)
#neg = read.table("S:/Projects/2018 R Text Mining/Negative.txt", sep="\n")
neg <- read.table("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\Negative.txt")
#View(neg)
pos.matches <- match(colnames(dtm),as.character(pos$V1))
neg.matches <- match(colnames(dtm),as.character(neg$V1))
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
pos.matrix <- doctmtrx[,pos.matches]
neg.matrix <- doctmtrx[,neg.matches]
sentiment.score <- rowSums(as.matrix(pos.matrix)) - rowSums(as.matrix(neg.matrix))
sentiment.score
New_data=NULL
New_data=cbind.data.frame(Doc_name=datafr[,c(1)],Sentiment_BOW=sentiment.score)
# check
head(New_data)
# legend
tbl_df(as.data.frame(New_data[1]))
# Sentiment plot by document
plot(x=row.names(New_data),y=New_data$Sentiment_BOW, type="b",xlab = "Document ID", ylab = "Sentiment")
# Alternate sentiment plot by document (superceded by next plot)
#ggplot(New_data, aes(x=row.names(New_data), y=Sentiment_BOW)) + geom_bar(stat="Identity", fill="Maroon")
# Latest sentiment plot by document
ggplot(New_data, aes(x=reorder(Doc_name,Sentiment_BOW), y=Sentiment_BOW)) + geom_bar(stat = "identity", fill="orange") + xlab("File name") + ylab("Sentiment Score") +
ggtitle("BOW Sentiment Score") + coord_flip()
## Using Syuzhet methodology ##
#install.packages('syuzhet')
library(syuzhet)
#install.packages('RNewsflow')
#install.packages('reshape2')
#install.packages('corrgram')
library(RNewsflow)
library(reshape2)
library(corrgram)
syuzhet_vector=get_sentiment(datafr$text,method="syuzhet")
New_data=cbind(New_data, Sentiment_syuzhnet=syuzhet_vector)
# check
#View(New_data)
s_v <- get_sentences(datafr$text)
s_v_sentiment <- get_sentiment(s_v)
nrc_data <- get_nrc_sentiment(datafr$text)
#View(nrc_data)
New_data=cbind.data.frame(New_data,nrc_data)
#glimpse(New_data)
#write.csv(New_data,file = "Sentiment_Analysis_Combined.csv")
# ADVANCED REQUIREMENT 3: Similarity among Documents
# we will visualize with a heatmap
as.data.frame(New_data)
# load the packages all in one go
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
lapply(x, require, character.only = TRUE)
doParallel::registerDoParallel(cores = 4)   # loads doParallel package
# setup for working directory
#setwd(choose.dir())  # alternate way to choose the folder
setwd("C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\contracts_test")
# setup for working directory
#setwd(choose.dir())  # alternate way to choose the folder
setwd("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\contracts_test")
getwd()
## Setup for special functions used in the analysis
# custom function to strip empty columns.  used in the sentiment analysis section below
has_data <- function(x) { sum(!is.na(x)) > 0 }
# set folder to read
folder_to_read <- "S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\contracts_test"
# check
folder_to_read
#
file_list <- list.files(
path = "S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\contracts_test",
#folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".txt", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
# set folder to read
folder_to_read <- "S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\contracts_test"
# check
folder_to_read
#
file_list <- list.files(
path = "S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\contracts_test",
#folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
pattern = ".pdf", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
all.files = FALSE,
full.names = FALSE,
recursive = TRUE,
ignore.case = FALSE,
include.dirs = TRUE,
no.. = FALSE)
# List of files that were read in
file_list
# reads in the files using the readtext package
datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object
# reads in the files using the readtext package
datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object
rt7 <- readtext(paste0("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\contracts_test\\*.pdf"))
head(rt7)
(rt7 <- readtext(paste0("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\contracts_test\\*.pdf"), docvarsfrom = "filenames",
docvarnames = c("document", "language")))
(rt7 <- readtext(paste0("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\contracts_test\\*.pdf"), docvarsfrom = "filenames",
docvarnames = c("document")))
View(rt7)
str(rt7)
