---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r include=FALSE}

# load the packages all in one go
x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
lapply(x, require, character.only = TRUE)

doParallel::registerDoParallel(cores = 4)   # loads doParallel package    

```

```{r include=FALSE}  
# setup for working directory  
#setwd(choose.dir())  # alternate way to choose the folder
setwd("C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts")
getwd()

```

```{r Custom functions, include=FALSE}
## Setup for special functions used in the analysis
# custom function to strip empty columns.  used in the sentiment analysis section below
has_data <- function(x) { sum(!is.na(x)) > 0 }

```

```{r List and process files, include=FALSE}

# set folder to read
folder_to_read <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts"

# check
folder_to_read

# 
file_list <- list.files(
  path = "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts",
  #folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
  pattern = ".txt", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
  all.files = FALSE, 
  full.names = FALSE, 
  recursive = TRUE, 
  ignore.case = FALSE, 
  include.dirs = TRUE, 
  no.. = FALSE)

```

### Files included in this analysis
```{r}
# List of files that were read in
file_list

```

```{r include=FALSE}

# reads in the files using the readtext package
datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object

# check
#str(datalist)  # datalist is a list of data frames
#head(datalist)  # shows the documents as 1x2 data frames

# combine all the datalist items into a single object
datafr = do.call("rbind", datalist) 

```

```{r Corpus transformations, include=FALSE}    

# creates a data corpus from the text column of the datafr data frame
datacorpus=Corpus(VectorSource(datafr$text))

## start of transformations (all done on datacorpus object)
jeopCorpus=NULL

removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)

jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
jeopCorpus <- tm_map(jeopCorpus, tolower)
jeopCorpus <- tm_map(jeopCorpus,removePunctuation)
#jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeMostPunctuation),preserve_intra_word_dashes = TRUE) 
jeopCorpus <- tm_map(jeopCorpus, removeNumbers)
jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeURL))
jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords('english'))
jeopCorpus <- tm_map(jeopCorpus, stripWhitespace)
#jeopCorpus <- tm_map(jeopCorpus, stemDocument,language="english")

```

```{r include=FALSE}
# make the document term matrix using this object
doctmtrx=DocumentTermMatrix(jeopCorpus)  # JKJ 2018/05/17 ~ changed from jeopCorpus.copy

# TO DO ... document what this does
dtm <- as.matrix(doctmtrx)
```

```{r include=FALSE}
## PREP

# Term summary (done on dtm object)
word_data_1=as.data.frame(cbind.data.frame(Words = colnames(dtm), freq=colSums(dtm)))
#names(word_data_1)

# put in descending order
word_data_1 <- word_data_1[order(-word_data_1$freq),]

# check  
word_data_1

# Optional: write to .csv  
#write.csv(word_data_1,"Summary.csv",row.names=F)
```

### WORDCLOUD
```{r}
# Wordcloud
wordcloud(words=colnames(dtm),freq=colSums(dtm), max.words = 30, min.freq=5, scale=c(4, .1), colors=brewer.pal(n=10, "Paired"), random.order=TRUE)
```

```{r}

# list most frequent terms (all documents)
(top_n_terms <- word_data_1 %>%
  select_if(has_data) %>% # strip empty columns
  top_n(10));

# new way (descending order)
ggplot(top_n_terms, aes(x=reorder(Words,freq), y=freq)) + geom_bar(stat = "identity", fill="red") + xlab("Words") + ylab("Count") + ggtitle("Top 10 Terms") + coord_flip()

```

## BIGRAMS
```{r include=FALSE}

bigrams = textcnt(datafr$text, n = 2, method = "string")
bigrams = bigrams[order(bigrams, decreasing = TRUE)]

# Optional: Write list of bigrams to a file
#fwrite(as.data.frame(bigrams),row.names = T,file = "Bigrams.csv") #This is to write the Bigram Summmary into an excel file.

# plot bar chart of bigrams
library(data.table)
d1=setDT(as.data.frame(bigrams), keep.rownames = TRUE)[]
```

```{r}

(top_n_bigrams <- d1 %>%
  select_if(has_data) %>% # strip empty columns
  top_n(10));

ggplot(d1[1:10], aes(x=reorder(rn,bigrams), y=bigrams)) + geom_bar(stat = "identity", fill="Blue") + xlab("Words") + ylab("Count") + ggtitle("Top Bigrams") + coord_flip()
```

## TRIGRAMS
```{r include=FALSE}

trigrams = textcnt(datafr$text, n = 3, method = "string")
trigrams = trigrams[order(trigrams, decreasing = TRUE)]

# View trigrams
#head(as.data.frame(trigrams), 20)

# Optional: Write list of trigrams to a file
#fwrite(as.data.frame(trigrams),row.names = T,file = "Trigrams.csv") #This is to write the Trigram Summmary into an excel file.

# plot bar chart of trigrams
d2=setDT(as.data.frame(trigrams), keep.rownames = TRUE)
```

```{r}

(top_n_trigrams <- d2 %>%
  select_if(has_data) %>% # strip empty columns
  top_n(10));

ggplot(d2[1:10], aes(x=reorder(rn,trigrams), y=trigrams)) + geom_bar(stat = "identity", fill="Purple") + xlab("Words") + ylab("Count") + ggtitle("Top Trigrams") + coord_flip()    

```

## DOCUMENT SCANNING --- User-Specified Keywords

```{r include=FALSE}   

# use the VCorpus object loaded via tm package
quantedaCorpus <- corpus(jeopCorpus)
```

```{r}

# specify first keyword
words <- c("China")

# show the words in context      
kwic(quantedaCorpus, words)

# specify second keyword

words <- c("Europe")
kwic(quantedaCorpus, words)

```

## DOCUMENT SCANNING - Audit Clause Keywords

```{r}
# specify first keyword
audit_clause <- c("records", "such records", "a period", "audit", "right")
kwic(quantedaCorpus, audit_clause)

```

## SENTIMENT ANALYSIS

```{r include=FALSE}

####### Using Bag of Words and calculating the overall Sentiment of all the document ####################
#pos = read.table("S:/Projects/2018 R Text Mining/Positive.txt", sep="\n")

pos <- read.table("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\Positive.txt")

#View(pos)

#neg = read.table("S:/Projects/2018 R Text Mining/Negative.txt", sep="\n")

neg <- read.table("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\Negative.txt")

#View(neg)

pos.matches <- match(colnames(dtm),as.character(pos$V1))
neg.matches <- match(colnames(dtm),as.character(neg$V1))
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
pos.matrix <- doctmtrx[,pos.matches]
neg.matrix <- doctmtrx[,neg.matches]

sentiment.score <- rowSums(as.matrix(pos.matrix)) - rowSums(as.matrix(neg.matrix))
sentiment.score
New_data=NULL
New_data=cbind.data.frame(Doc_name=datafr[,c(1)],Sentiment_BOW=sentiment.score)

# check
  head(New_data)

```    

### Sentiment Score by Document

```{r}
# legend
tbl_df(as.data.frame(New_data[1]))

# Sentiment plot by document
plot(x=row.names(New_data),y=New_data$Sentiment_BOW, type="b",xlab = "Document ID", ylab = "Sentiment")

# Alternate sentiment plot by document (superceded by next plot)
#ggplot(New_data, aes(x=row.names(New_data), y=Sentiment_BOW)) + geom_bar(stat="Identity", fill="Maroon")

# Latest sentiment plot by document
ggplot(New_data, aes(x=reorder(Doc_name,Sentiment_BOW), y=Sentiment_BOW)) + geom_bar(stat = "identity", fill="orange") + xlab("File name") + ylab("Sentiment Score") + 
  ggtitle("BOW Sentiment Score") + coord_flip()


```

```{r include=FALSE}    

## Using Syuzhet methodology ##

#install.packages('syuzhet')
library(syuzhet)

#install.packages('RNewsflow')
#install.packages('reshape2')
#install.packages('corrgram')
library(RNewsflow)
library(reshape2)
library(corrgram)

syuzhet_vector=get_sentiment(datafr$text,method="syuzhet")
New_data=cbind(New_data, Sentiment_syuzhnet=syuzhet_vector)

# check
#View(New_data)

s_v <- get_sentences(datafr$text)
s_v_sentiment <- get_sentiment(s_v)

nrc_data <- get_nrc_sentiment(datafr$text)
#View(nrc_data)

New_data=cbind.data.frame(New_data,nrc_data)
#glimpse(New_data)
#write.csv(New_data,file = "Sentiment_Analysis_Combined.csv")

# ADVANCED REQUIREMENT 3: Similarity among Documents

# we will visualize with a heatmap

```

# View Sentiment results table

```{r}
as.data.frame(New_data)

```

```{r}

doc_comp <- as.data.frame(documents.compare(doctmtrx ))

# legend
tbl_df(as.data.frame(New_data[1]))

doc_comp <- doc_comp %>%
  arrange(desc(similarity))

doc_comp

new_doc_comp <-  reshape2::acast(doc_comp, x  ~ y, value.var = "similarity")
new_doc_comp.sorted = new_doc_comp[as.character(order(rownames(new_doc_comp))),as.character(order(rownames(new_doc_comp)))]

rm(new_doc_comp)

new_doc_comp.sorted <- as.data.frame(new_doc_comp.sorted)

# corrgram
corrgram(new_doc_comp.sorted,
         #order=TRUE,
         main="Document Similarity Matrix",
         lower.panel=panel.shade,
         upper.panel=panel.pie,
         diag.panel = panel.pts,
         text.panel=panel.txt)

corrgram(new_doc_comp.sorted,
         lower.panel=panel.pts, upper.panel=panel.conf,
         diag.panel=panel.density)

```   