---
title: "R Notebook"
output:
  html_notebook: 
    fig_caption: yes
    fig_height: 5
    fig_width: 10
    number_sections: yes
    theme: readable
    toc: yes
  word_document: default
  html_document:
    df_print: paged
---

```{r include=FALSE}
#### Load required packages and set directory ####

# load the packages all in one go
  x <- c("RODBC", "data.table", "readtext", "tm", "wordcloud", "RWeka", "tidytext", "doParallel", "tau", "quanteda", "stringr", "ggplot2", "dplyr")
  lapply(x, require, character.only = TRUE)

  doParallel::registerDoParallel(cores = 4)   # loads doParallel package    

```

```{r, include=FALSE}  
# setup for working directory  
  #setwd(choose.dir())  # alternate way to choose the folder
  setwd("C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts")

# Setup for database connection (use only if needed)
  #server1 <-odbcDriverConnect('driver={SQL Server};server=CQW-DBS001236.stage.twi.com; database=IAACE1; uid=IAACE1User; pwd=9n5B1RIc')

## Setup for special functions used in the analysis
  # custom function to strip empty columns.  used in the sentiment analysis section below
    has_data <- function(x) { sum(!is.na(x)) > 0 }
```

```{r, include=FALSE}
#### Folder control and reading in data ####
  
  # set folder to read
    folder_to_read <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts"

  # check
    folder_to_read

  # 
    file_list <- list.files(
      path = folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
      pattern = ".txt", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
      all.files = FALSE, 
      full.names = FALSE, 
      recursive = TRUE, 
      ignore.case = FALSE, 
      include.dirs = TRUE, 
      no.. = FALSE)
```

```{r}
  # check that all files were read in
    file_list   # Returns the files in the folder
```

```{r include=FALSE}
# TO DO: ignore for now
# make any adjustments to file_list after checking (i.e. remove unwanted files)
```

```{r include=FALSE}

  # set folder to read
    folder_to_read <- "C:\\Users\\jonesjosh\\OneDrive - Time Warner\\Code_Files\\R_Scripts\\Development\\R__Text Mining Project 2018\\texts"

  # check
    folder_to_read

  # 
    file_list <- list.files(
      path = folder_to_read, # JKJ ~ need to fix so that we can let user specify file path
      pattern = ".txt", # JKJ 2018/05/09 ~ need to make sure we can read Word and PDF too
      all.files = FALSE, 
      full.names = FALSE, 
      recursive = TRUE, 
      ignore.case = FALSE, 
      include.dirs = TRUE, 
      no.. = FALSE)

  # reads in the files using the readtext package
    datalist <- lapply(file_list, function(x) readtext(x))  # datalist is a list object

  # check
    #str(datalist)  # datalist is a list of data frames
    #head(datalist)  # shows the documents as 1x2 data frames

    # combine all the datalist items into a single object
      datafr = do.call("rbind", datalist) 

      rm(datalist)  # clean up, datalist has been replaced with datafr

  # check
    #head(datafr)  # datafr is a data frame with two columns: doc_id and text
    str(datafr)
    #datafr$text[7]
    #write.table(datafr$text[7],"test.txt")
```
      
```{r include=FALSE}      
#### Cleaning the text and creating the corpus ####

  # test for replacing garbage strings / characters  
    gsub("the", "", datafr$text)
    gsub("â", "", datafr$text)

    # clear the console
      cat("\014")
    
  # creates a data corpus from the text column of the datafr data frame
    datacorpus=Corpus(VectorSource(datafr$text))

    # Checking the corpus

      # check
      for (i in 1:5) {
        cat(paste("[[", i, "]] ", sep = ""))
        writeLines(as.character(datacorpus[[i]]))
      }

#### TESTING SECTION - data frame ####

  # make a data frame out of datafr for future use
    df1 <- as.data.frame(datafr) %>%
      gsub("the", "", datafr$text) %>%
      gsub("â", "", datafr$text)

# JKJ 2018/05/08 ~ added this option that reads in the files directly to a corpus
# tm pkg
#datafr_corpus <- VCorpus(DirSource(getwd()))
#rm(datafr_corpus)  # clean up

# TESTING
# convert corpus back to data frame
#dataframe <- data.frame(text=sapply(datafr_corpus, identity), stringsAsFactors=F)
#rm(dataframe)  # clean up

```    

```{r include=FALSE}    
#### Transformations on the corpus ####
  
## start of transformations (all done on datacorpus object)
  jeopCorpus=NULL
  
  removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
  
  jeopCorpus <- tm_map(datacorpus, PlainTextDocument)
  jeopCorpus <- tm_map(jeopCorpus, tolower)
  jeopCorpus <- tm_map(jeopCorpus,removePunctuation)
  #jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeMostPunctuation),preserve_intra_word_dashes = TRUE) 
  jeopCorpus <- tm_map(jeopCorpus, removeNumbers)
  jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeURL))
  jeopCorpus <- tm_map(jeopCorpus, removeWords, stopwords('english'))
  jeopCorpus <- tm_map(jeopCorpus, stripWhitespace)
  #jeopCorpus <- tm_map(jeopCorpus, stemDocument,language="english")

## TO DO ... need to remove user-specified words / phrases ... 
  removeGarbage <- function(x) gsub("â", "", x)
  jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeGarbage))
  removeGarbage <- function(x) gsub("???T", "", x)
  jeopCorpus <- tm_map(jeopCorpus, content_transformer(removeGarbage))
```
  
```{r include=FALSE}  
  # check
    for (i in 1:5) {
      cat(paste("[[", i, "]] ", sep = ""))
      writeLines(as.character(jeopCorpus[[i]]))
    }    
```

```{r include=FALSE}
#### Document term matrix for analysis ####

  # create a copy
    jeopCorpus.copy=jeopCorpus
  
  # previous transformations turned the corpus of texts to character.  now we need to change back to a vector source    
    jeopCorpus.copy <- Corpus(VectorSource(jeopCorpus.copy))
  
  # declare the doctmtrx object
    doctmtrx=NULL
  
  # make the document term matrix using this object
    doctmtrx=DocumentTermMatrix(jeopCorpus)  # JKJ 2018/05/17 ~ changed from jeopCorpus.copy
  
  # TO DO ... document what this does
    alarm()
  
  # check
    class(doctmtrx)
  
  # TO DO ... document what this does
    dtm <- as.matrix(doctmtrx)
  
  # check
    length(doctmtrx)
    length(colSums(as.matrix(doctmtrx)))
  
  # clean up
    #rm(jeopCorpus.copy)
    #rm(doctmtrx)
```
```{r include=FALSE}
#### Beginning of analysis section ####

## PREP

    # Term summary (done on dtm object)
      word_data_1=as.data.frame(cbind.data.frame(Words = colnames(dtm), freq=colSums(dtm)))
      #names(word_data_1)

    # put in descending order
      word_data_1 <- word_data_1[order(-word_data_1$freq),]

    # check  
      word_data_1

    # Optional: write to .csv  
      #write.csv(word_data_1,"Summary.csv",row.names=F)
```

### WORDCLOUD
```{r}

# Wordcloud
wordcloud(words=colnames(dtm),freq=colSums(dtm), max.words = 30, min.freq=5, scale=c(4, .1), colors=brewer.pal(n=10, "Paired"), random.order=TRUE)

```

### TERM SUMMARIES
```{r}


#### to do in this section
  # 1. still need to list final step --- top n for EACH document
    
  # list most frequent terms (all documents)
    top_n_terms <- word_data_1 %>%
    select_if(has_data) %>% # strip empty columns
    top_n(10)  

    top_n_terms

    # old way
    #ggplot(top_n_terms, aes(x=Words, y=freq)) + geom_bar(stat = "identity") + xlab("Words") + ylab("Count") +coord_flip()
  
    # new way (descending order)
    ggplot(top_n_terms, aes(x=reorder(Words,freq), y=freq)) + geom_bar(stat = "identity", fill="red") + xlab("Words") + ylab("Count") + ggtitle("Top 10 Terms") + coord_flip()
    
  # list most frequent terms (by document)
    #as.data.frame(findMostFreqTerms(doctmtrx, n=10))
```
## BIGRAMS and TRIGRAMS
```{r}

  ### BIGRAMS
  bigrams = textcnt(datafr$text, n = 2, method = "string")
  bigrams = bigrams[order(bigrams, decreasing = TRUE)]

  # need a way to show top 10 bigrams per document (similar to top 10 terms above)

  # View bigrams
  #head(as.data.frame(bigrams), 20)

  # Optional: Write list of bigrams to a file
    #fwrite(as.data.frame(bigrams),row.names = T,file = "Bigrams.csv") #This is to write the Bigram Summmary into an excel file.

  # plot bar chart of bigrams
  library(data.table)
  d1=setDT(as.data.frame(bigrams), keep.rownames = TRUE)[]
  
  
  top_n_bigrams <- d1 %>%
  select_if(has_data) %>% # strip empty columns
  top_n(10)  
  
  top_n_bigrams
  
  ggplot(d1[1:10], aes(x=reorder(rn,bigrams), y=bigrams)) + geom_bar(stat = "identity", fill="Blue") + xlab("Words") + ylab("Count") + ggtitle("Top Bigrams") + coord_flip()
  
### TRIGRAMS
  trigrams = textcnt(datafr$text, n = 3, method = "string")
  trigrams = trigrams[order(trigrams, decreasing = TRUE)]

  # View trigrams
    #head(as.data.frame(trigrams), 20)

  # Optional: Write list of trigrams to a file
    #fwrite(as.data.frame(trigrams),row.names = T,file = "Trigrams.csv") #This is to write the Trigram Summmary into an excel file.

  # plot bar chart of trigrams
    d2=setDT(as.data.frame(trigrams), keep.rownames = TRUE)[]
    
    ggplot(d2[1:10], aes(x=reorder(rn,trigrams), y=trigrams)) + geom_bar(stat = "identity", fill="Purple") + xlab("Words") + ylab("Count") + ggtitle("Top Trigrams") + coord_flip()    

```
```{r include=FALSE}        
#### Advanced Requirements ####

# quanteda package (loaded during setup)

# use the VCorpus object loaded via tm package
  quantedaCorpus <- corpus(jeopCorpus)
  
    #quantedaCorpus <- tm_map(quantedaCorpus, tolower)
    #quantedaCorpus <- corpus(VectorSource(quantedaCorpus))
  
  # TO DO: figure out how to use the same corpus that we already cleaned / did the transformations to here
  
  # clean up

  # check structure
    #summary(quantedaCorpus)
```

```{r}
### ADVANCED REQUIREMENT 1: Perform a search for a word across the corpuses, and view the contexts in which it occurs

  # specify first keyword
    words <- c("China")

  # show the words in context      
    kwic(quantedaCorpus, words)
    
  # specify second keyword
    
    words <- c("People")
    kwic(quantedaCorpus, words)

    
```

### ADVANCED REQUIREMENT 2: Sentiment Analysis
```{r include=FALSE}
  # We will use Tarun's method for sentiment analysis using the two approaches he outlines
  # 1.	Using Bag of words - From a reference files of positive words and negative words , I have calculated Sentiement_BOW.
  # 2.	Using Syuzhet Methodology - I have used this to present various emotions and rate each speech accordingly. Also, I have lastly categorized into Positive and Negative sentiments using "NRC" technique. 

  # viz results
  # we will visualize in two ways
    # 1. the table similar to tarun's email file
    # 2. heatmap

####### Using Bag of Words and calculating the overall Sentiment of all the document ####################
    #pos = read.table("S:/Projects/2018 R Text Mining/Positive.txt", sep="\n")
    
    pos <- read.table("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\Positive.txt")
    
    #View(pos)

    #neg = read.table("S:/Projects/2018 R Text Mining/Negative.txt", sep="\n")
  
    neg <- read.table("S:\\ACE\\Projects\\ACE - Ongoing and Standard Analytics\\R__Text Mining\\Negative.txt")
    
    #View(neg)

    pos.matches <- match(colnames(dtm),as.character(pos$V1))
    neg.matches <- match(colnames(dtm),as.character(neg$V1))
    pos.matches <- !is.na(pos.matches)
    neg.matches <- !is.na(neg.matches)
    pos.matrix <- doctmtrx[,pos.matches]
    neg.matrix <- doctmtrx[,neg.matches]
    
    sentiment.score <- rowSums(as.matrix(pos.matrix)) - rowSums(as.matrix(neg.matrix))
    sentiment.score
    New_data=NULL
    New_data=cbind.data.frame(Doc_name=datafr[,c(1)],Sentiment_BOW=sentiment.score)
    
```    

```{r}
# legend
tbl_df(as.data.frame(New_data[1]))

# Sentiment plot by document
plot(x=row.names(New_data),y=New_data$Sentiment_BOW, type="b",xlab = "Document ID", ylab = "Sentiment")

# Alternate sentiment plot by document
ggplot(New_data, aes(x=row.names(New_data), y=Sentiment_BOW)) + geom_bar(stat="Identity", fill="Maroon")

```

```{r include=FALSE}    
############## Using Syuzhet methodology ##################
#install.packages('syuzhet')
library(syuzhet)
#install.packages('RNewsflow')
#install.packages('reshape2')
#install.packages('corrgram')
library(RNewsflow)
library(reshape2)
library(corrgram)

syuzhet_vector=get_sentiment(datafr$text,method="syuzhet")
New_data=cbind(New_data, Sentiment_syuzhnet=syuzhet_vector)

  # check
    #View(New_data)

s_v <- get_sentences(datafr$text)
s_v_sentiment <- get_sentiment(s_v)

nrc_data <- get_nrc_sentiment(datafr$text)
  #View(nrc_data)

New_data=cbind.data.frame(New_data,nrc_data)
  #glimpse(New_data)
  #write.csv(New_data,file = "Sentiment_Analysis_Combined.csv")

# ADVANCED REQUIREMENT 3: Similarity among Documents

# we will visualize with a heatmap


```

```{r}
# View Sentiment results table
as.data.frame(New_data)
```

```{r tidy=TRUE fig.align=center}

doc_comp <- as.data.frame(documents.compare(doctmtrx ))

# legend
tbl_df(as.data.frame(New_data[1]))

doc_comp <- doc_comp %>%
    arrange(desc(similarity))

doc_comp

new_doc_comp=reshape2::acast(doc_comp, x  ~ y, value.var = "similarity")
new_doc_comp <- as.data.frame(new_doc_comp)

corrgram(new_doc_comp)
```    